\lesson{original Lecture 10 -- 12}{Hypothesis Testing}

\subsection{Model Setup}

We assume that the data are sampled from $X\sim{p_\theta}$,
where $p_\theta\in\mathcal{P}=\{p_\theta:\theta\Omega\}$.
We divide the models in $\mathcal{P}$ into two disjoint sub-classes known as hypothesis:
\begin{align}
    H_0:&~\theta\in\Omega_0\subset{\Omega}~\text{(null hypothesis)}\\
    H_1:&~\theta\in\Omega_1=\Omega\setminus\Omega_0~\text{(alternative hypothesis)}
\end{align}

Our goal is to infer (from the data) which hypothesis is ``correct''.
Our decision space is $\mathcal{D}=\{\text{accept}~H_0,~\text{reject}~H_1\}$.
The loss function of testing is shown as the confusion matrix Table \ref{tab:testconfmtx},
where $\beta$ is power.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Confusion matrix of testing's loss function $L(\theta,d)$}
    \begin{tabular}{cc|cc}
          &       & \multicolumn{2}{c}{Truth} \\
          &       & $\theta\in\Omega_0$ ($H_0$ is true) & $\theta\in\Omega_1$ ($H_0$ is false) \\
    \midrule
    \multirow{2}[1]{*}{Decision}
          & Reject $H_0$ & $\alpha$ (Type I error) & 0 \\
          & Accept $H_0$ & 0     & $1-\beta$ (Type II error) \\
    \end{tabular}
  \label{tab:testconfmtx}%
\end{table}%

We make decision with testing function (or critical function) $\varphi(X)\in[0,1]$,
which indicates that $\delta_\varphi(X,U)$ rejects $H_0$ with probability $\varphi(X)$, i.e.
\begin{gather}
    \varphi(X)=P(\delta_\varphi(X,U)=\text{Reject}~H_0|X)
\end{gather}
where $U$ \rotatebox{90}{$\models$} $X$ is randomized component.

\begin{definition}[Power function]
    The power function of a test $\varphi$ is 
    \begin{gather}
        \beta(\theta)=\mathbb{E}_\theta\varphi(X)=P_\theta(\text{Reject}~H_0)
    \end{gather}
    If $\theta\in\Omega_0$, then $\beta(\theta)=R(\theta,\delta_\varphi)=\text{Type I error}~(\alpha);$\\
    If $\theta\in\Omega_1$, then $\beta(\theta)=1-R(\theta,\delta_\varphi)=1-\text{Type II error}$.
\end{definition}

Our ideal goal is to minimize $\beta(\theta_0)$ uniformly for all $\theta\in\Omega_0$
and maximize $\beta(\theta)$ uniformly for all $\theta\in\Omega_1$.

\subsection{Neyman-Pearson Paradigm}

Fix $\alpha\in(0,1)$ to control the Type I error. (level of significance)
We require that all procedure can satisfy the following risk bound:
\begin{align}
    \sup_{\theta\in\Omega_0}\mathbb{E}_\theta\varphi(X) 
    = \sup_{\theta\in\Omega_0}\beta(\theta)\leq\alpha.
\end{align}
And our optimality goal is to find a level $\alpha$ test that maximises the power
$\beta(\theta)=\mathbb{E}_\theta\varphi(X)$ for each $\theta\in\Omega_1$.
Such a test is called uniformly most powerful test (UMP).

\subsection{Simple vs Simple}

MP test for the ``Simple vs Simple'' case.

\begin{definition}[Simple vs simple]
    A hypothesis $H_0$ is called simple if $|\Omega_0|=1$,
    otherwise it is called composite.
    The same is the for $H_1$.
\end{definition}


For the simple versus simple case, will adopt the notation:
\begin{align}
    H_0:&~X\sim{p_0}~~~(\theta=\theta_0)\\
    H_1:&~X\sim{p_1}~~~(\theta=\theta_1)
\end{align}
Our goal can be compactly described as following with predefined $\alpha$:
\begin{gather}
    \max_\varphi\mathbb{E}_{p_1}\varphi(X)\\
    \text{s.t.}~\mathbb{E}_{p_0}\varphi(X)\leq{\alpha}
\end{gather}

\begin{lemma}\label{lem:nplemma}
    \textbf{Neyman-Pearson Lemma}\\
    \begin{enumerate}[{(i)}]
        \item \textbf{Existence}\\
        For testing $H_0:p_0$ vs $H_1:p_1$,
        there is a test $\varphi(X)$ and a constant $k$ such that
        \begin{enumerate}[{a)}]
            \item $\mathbb{E}_{p_0}\varphi(X)=\alpha$ (size$=$level)
            \item $\varphi(x)=\left\{\begin{array}{ll}
                1 & \text{if}~\frac{p_1(x)}{p_0(x)}>k_\alpha \\
                0 & \text{if}~\frac{p_1(x)}{p_0(x)}<k_\alpha
            \end{array}\right.$,
            where $\frac{p_1(x)}{p_0(x)}>k_\alpha$ is rejection (or critical) region, 
            i.e. rejecting $H_0$ if $\frac{p_1(x)}{p_0(x)}>k_\alpha$.
            This test is called a \textbf{likelihood ratio test}.
        \end{enumerate}
        \item \textbf{Sufficiency}\\
        If a test statistics (a) and (b) for some constant $k_\alpha$,
        it is most powerful for testing $H_0:p_0$ vs $H_1:p_1$ at level $\alpha$.
        \textbf{$\varphi$ is MP.}
        \item \textbf{Necessity}\\
        If a test $\varphi$ is MP at level $\alpha$,
        then it satisfies (b) for some $k_\alpha$ and it also satisfies (a)
        unless there exists a test of size $<\alpha$ with power $1$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    skipped, refer to (Keener, 2010) if it is needed.
\end{proof}

\begin{example}
    Let $X_1,\cdots,X_n\overset{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$ with $\sigma^2$ known.
    Consider the two hypotheses: $H_0:\mu=0$ vs $H_1:\mu=\mu_1$ ($\mu_1>0$ known).
    \begin{align}
        L(\boldsymbol{x})
        =& \frac{p_1(\boldsymbol{x})}{p_0(\boldsymbol{x})}
        = \frac{\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(x_i-\mu_1)^2}{2\sigma^2}\right\}}
        {\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{x_i^2}{2\sigma^2}\right\}}\\
        =& \exp\left\{\frac{\mu_1}{\sigma^2}{\sum_{i=1}^n{x_i}}-\frac{n\mu_1^2}{2\sigma^2}\right\}
    \end{align}
    We have the following observeation:
    \begin{align}
        L(\boldsymbol{x})>k
        \Rightarrow& \frac{\mu_1}{\sigma^2}{\sum_{i=1}^n{x_i}}-\frac{n\mu_1^2}{2\sigma^2}>\log{k}\\
        \Rightarrow& \mu_1\sum_{i=1}^n{x_i}>k'\\
        \Rightarrow& \left\{\begin{array}{ll}
            \sum_{i=1}^n{x_i}>k'' & \text{if}~\mu_1>0 \\
            \sum_{i=1}^n{x_i}<k'' & \text{if}~\mu_1<0
        \end{array}\right.
    \end{align}
    Let's focus on the first case when $\mu_1>0$.
    We can rewrite our test in a different form as follows:
    \begin{gather}
        \frac{\sqrt{n}\Bar{x}_n}{\sigma}>k'''.
    \end{gather}
    By NP Lemma, we reject $H_0$ if and only if $\frac{\sqrt{n}\Bar{x}_n}{\sigma}>k_\alpha$ is MP,
    where $k'''=k(\alpha)=z_{1-\alpha}$ is the $1-\alpha$ quantitle of $\mathcal{N}(0,1)$.
\end{example}


\begin{definition}[Power of testing]
    For simple $H_0:p_0$ vs $H_1:p_1$,
    we call $\beta_\varphi(p_1)=\mathbb{E}_{p_1}\varphi(X)$ the power of $\varphi$,
    i.e. the probability of rejecting $H_0$ under the alternative hypothesis.
\end{definition}

\begin{corollary}
    Suppose $\beta$ is the power of a most powerful level $\alpha$ test of $H_0:p_0$ vs $H_1:p_1$ with $\alpha\in[0,1)$,
    then $\alpha<\beta$ unless $p_0=p_1$.\footnote{TSH 3.2.1}
    That is saying this MP test rejects more often under $H_1$ then $H_0$.
\end{corollary}
\begin{proof}
    Consider the test $\varphi_0(X)\equiv\alpha$, 
    which always rejects with probability $\alpha$.
    Since $\varphi_0$ is level $\alpha$ and $\beta$ is the maximal pwer, we have
    \begin{gather}
        \beta\geq\mathbb{E}_{p_1}\varphi_0(X)=\alpha.
    \end{gather}
    Suppose $\beta=\alpha$, then $\varphi_0(X)=\alpha$ is a most powerful level $\alpha$ text.
    As a result, by Lemma \ref{lem:nplemma}, NP Lemma, (iii),
    \begin{gather}
        \varphi_0(x)=\left\{\begin{array}{ll}
            1 & \text{if}~\frac{p_1(x)}{p_0(x)}>k, \\
            0 & \text{if}~\frac{p_1(x)}{p_0(x)}<k.
        \end{array}\right.
    \end{gather}
    Since $\varphi_0(X)$ never equals 0 or 1, 
    it must be the case that $p_1(x)=kp_0(x)$ with probability 1.
    Note that 
    \begin{gather}
        \int{p_1(x)}d\mu(x)=k\int{p_0}d\mu(x)=1,
    \end{gather}
    which implies that $k=1$, and hence $p_0=p_1$.
\end{proof}

\subsection{Exponential Families and UMP One-side Tests}

\textbf{Goal}: Extend the MP tests for simple alternatives up to 
\textbf{uniformly most powerful} (UMP) tests for composite alternatives.

\begin{example}
    \textbf{One-parameter exponential family}\\
    Consider $X_1,\cdots,X_n\overset{iid}{\sim}p_\theta$,
    where $p_\theta(x)\propto h(x)\exp\{\theta{T(x)}\}$,
    and we are interested in testing 
    \begin{gather}
        H_0:\theta=\theta_0~ \text{vs} ~H_1:\theta=\theta_1
    \end{gather}.
    We want to construct an MP test at level $\alpha$.
    The corresponding likelihood ratio is 
    \begin{gather}
        \frac{\prod_{i=1}^n p_{\theta_1}(x_i)}{\prod_{i=1}^n p_{\theta_0}(x_i)}
        \propto
        \exp\left\{
            (\theta_1-\theta_0)\sum_{i=1}^n{T(x_i)}
        \right\}
    \end{gather}
    Assuming that $\theta_1>\theta_0$,
    we shall reject for large value of $\sum_{i=1}^n T(x_i)$.
    That is, an MP test has the following form:
    \begin{gather}
    \varphi(\boldsymbol{x})=\left\{\begin{array}{ll}
        1       & \text{if}~\sum_{i=1}^n{T(x_i)}>k, \\
        \gamma  & \text{if}~\sum_{i=1}^n{T(x_i)}=k, \\
        0       & \text{if}~\sum_{i=1}^n{T(x_i)}<k,
    \end{array}\right.
    \end{gather}
    where $k$ and $\gamma$ are chosen to satisfy the size constraint:
    \begin{align}
        \alpha=\mathbb{E}_{\theta_0}\varphi(X)
        = P_{\theta_0}\left(\sum_{i=1}^n T(X_i)>k\right)
        + \gamma P_{\theta_0}\left(\sum_{i=1}^n T(X_i)>k\right).
    \end{align}

    Note that $\sum_{i=1}^n T(X_i)$ has no explicit $\theta$ dependence
    and that $k$ and $\gamma$ do not depend on $\theta_1$ (with $\theta_1>\theta_0$).
    This means that $\varphi$ is in fact a UMP test for testing 
    \begin{gather}
        H_0:\theta=\theta_0~ \text{vs} ~H_1:\theta>\theta_0
    \end{gather}.
    Hence $H_1$ is an example of a one-sided alternative,
    which arises when the parameter values of interest lie on only one side of the real-valued parameter $\theta_0$.
\end{example}

\begin{definition}
    \textbf{Families with monotone likelihood ratio (MLR)}\\
    We say that the family of densities $\{p_\theta:\theta\in\mathbb{R}\}$ has monotone likelihood ratio inn $T(X)$ if
    \begin{enumerate}[{(1)}]
        \item $\theta\neq\theta'$ implies $p_{\theta}\neq p_{\theta'}$ (identifiability),
        \item $\theta<\theta'$ implies $\frac{p_{\theta'(x)}}{p_{\theta}(x)}$ is a non-decreasing function of $T(X)$ (monotonicity).
    \end{enumerate}
\end{definition}

\begin{example}
    $~$\\
    \begin{enumerate}[{1)}]
        \item $T(\boldsymbol{X})=\sum_{i=1}^n T(X_i)$ in the one-parameter exponential family.
        \item (Double exponential) Let $X\sim\text{DExp}(\theta)$ with density
        $p_\theta(x)=\frac{1}{2}\exp\{-|x-\theta|\}$.
        It is easy to see that the model is identifiable,
        so we need check only the condition (ii) in the definition of MLR.

        Fix any $\theta'>\theta$, and consider the likelihood ratio
        \begin{gather}
            \frac{p_{\theta'(x)}}{p_\theta(x)}
            =\exp\{|x-\theta|-|x-\theta'|\}.
        \end{gather}
        Note that 
        \begin{gather}
            |x-\theta|-|x-\theta'|=\left\{\begin{array}{cc}
                \theta-\theta'      & \text{if}~x<\theta \\
                2x-\theta-\theta'   & \text{if}~\theta\leq{x}\leq\theta \\
                \theta'-\theta      & \text{if}~x>\theta
            \end{array}\right.
        \end{gather}
        which is non-decreasing in $x$.
        Therefore, the family has MLR in $T(X)=X$.
    \end{enumerate}
\end{example}

\begin{example}
    \textbf{Cauchy location model}\\
    Let $X$ has density 
    \begin{gather}
        p_\theta=\frac{1}{\pi}\cdot\frac{1}{1+(x-\theta)^2}.
    \end{gather}
    We find two points for which the MLR conditions fails:
    For any fixed $\theta>0$,
    \begin{gather}
        \frac{p_\theta(x)}{p_0(x)}=\frac{1+x^2}{1+(x-\theta)^2}\to{1}~\text{as}~x\to\pm\infty.
    \end{gather}
    But $\frac{p_\theta(0)}{p_0(0)}=\frac{1}{1+\theta^2}<1$.
    Thus the ratio must increase at some values of $x$ and decrease at others.
    In particular, it is not monotone in $x$.
    Hence, we can conclude that the LR in $T(X)=X$ is not MLR.
\end{example}

\begin{theorem}
    Suppose $X\sim p_\theta$ has MLR in $T(X)$ and we test
    \begin{gather}
        H_0:\theta\leq\theta_0~\text{vs}~H_1:\theta>\theta_0,
    \end{gather}
    then\footnote{
    TSH 3.4.1
    }
    \begin{enumerate}[{(i)}]
        \item There exists a UMP test at level $\alpha$ of the form
        \begin{gather}
            \varphi(X)=\left\{\begin{array}{ll}
                1 & \text{if}~T(X)>k \\
                \gamma & \text{if}~T(X)=k \\
                0 & \text{if}~T(X)<k 
            \end{array}\right.
        \end{gather}
        where $k,\gamma$ are determined by the condition $\mathbb{E}_{\theta_0}\varphi(X)=\alpha$.
        \item The power function $\beta(\theta)=\mathbb{E}_\theta\varphi(X)$ is strictly increasing when $0<\beta(\theta)<1$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    refer to TSH
\end{proof}

\begin{note}
    Possible directions for constructing UMPs:
    \begin{enumerate}
        \item Reduce the composite alternative to a simple alternative
        \item Collapse the composite null to simple null
        \item Apply NP Lemma.
    \end{enumerate}
\end{note}

\subsection{Optimal Tests for Composite Nulls}

\begin{gather}\begin{array}{ll}
    H_0:X\sim f_\theta & \theta\in\Omega_0 \\
    H_1:X\sim g & g~\text{unknown}
\end{array}\end{gather}
We may impose a prior distribution $\Lambda$ on $\Omega_0$.
So, we consider the new hypothesis converting composite null to simple one:
\begin{gather}
    H_\Lambda:X\sim h_\Lambda(x)=\int_{\Omega_0}f_\theta(x)d\Lambda(\theta)
\end{gather}
where $h_\lambda(x)$ is the marginal distribution of $X$ induced by $\Lambda$.
Let $\beta_\Lambda$ be the power of the MP level-$\alpha$ test $\varphi_\Lambda$ for testing $H_\Lambda$ vs $g$.\footnote{
TSH 3.8
}

\begin{definition}
    \textbf{Least favourable distribution}\\
    $\Lambda$ is a least favourable distribution if $\beta_\Lambda\leq\beta_{\Lambda'}$ for any prior $\Lambda'$.
\end{definition}

\begin{theorem}\label{thm:convertcompsim}
    Suppose $\varphi_\Lambda$ is an MP level-$\alpha$ test for testing $H_\Lambda$ against g.
    If $\varphi_\Lambda$ is level-$\alpha$ for the original hypothesis $H_0$
    (i.e. $\mathbb{E}_{\theta_0}\varphi_\Lambda\leq\alpha~\forall\theta_0\in\Omega$), then\footnote{
    TSH 3.8.1
    }
    \begin{enumerate}[{(i)}]
        \item the test $\varphi_\Lambda$ is MP for the original test on $H_0:\theta\in\Omega_0$ vs $g$ and 
        \item the prior distribution $\Lambda$ is least favourable.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $~$\\
    \begin{enumerate}[{(i)}]
        \item Let $\varphi^*$ be any other level-$\alpha$ test of $H_0:\theta\in\Omega$ vs $g$.
        Then $\varphi^*$ is also a level-$\alpha$ test for $H_\Lambda$ vs $g$
        because
        \begin{gather}
            \mathbb{E}_\theta\varphi^*(X)=\int{\varphi^*(x)f_\theta(x)}d\mu(x)\leq\alpha~~~\forall{\theta}\in\Omega_0
        \end{gather}
        which implies that 
        \begin{gather}
            \int{\varphi^*(x)h_\Lambda(X)}d\mu(x)
            = \iint\varphi^*(x)f_\theta(x)d\mu(x)d\Lambda(\theta)
            \leq \int{\alpha}d\Lambda(\theta)
            =\alpha
        \end{gather}
        Since $\varphi_\Lambda$ is MP for $H_\Lambda$ vs $g$, we have 
        \begin{gather}
            \int\varphi^*g(x)d\mu(x)\leq\int{\varphi_\Lambda(x)g(x)}d\mu(x)
        \end{gather}
        Hence $\varphi_\Lambda$ is a MP test for $H_0$ vs $g$ because $\varphi_\Lambda$ is also level $\alpha$.
        \item Let $\Lambda'$ be any distribution on $\Omega_0$.
        Since $\mathbb{E}_\theta\varphi_\Lambda(X)\leq\alpha~\forall\theta\in\Omega_0$,
        we know that $\varphi_\Lambda$ must be level $\alpha$ for $H_\Lambda'$ vs $g$.
        Thus $\beta_\Lambda\leq\beta_{\Lambda'}$.
        So $\Lambda$ is the least favorable distribution.
    \end{enumerate}
\end{proof}

\begin{example}
    \textbf{Testing in the presence of nuisance parameters}\\
    Let $X_1,\cdots,X_n\overset{iid}{\sim}\mathcal{N}(\theta,\sigma^2)$
    where both $\theta$ and $\sigma^2$ are unknown.
    We consider testing $H_0:\sigma\leq\sigma_0$ against $H_1:\sigma>\sigma_0$.
    To find a UMP test,
    we follow the strategy discussed in the previous lecture:
    \begin{enumerate}
        \item First, we fix a simple alternative ($\theta_1,\sigma_1$) for some arbutrary $\theta_1$ and $\sigma_1>\sigma_0$.
        \item We choose a prior distribution $\Lambda$ to collapse our null hypothesis over.
        Intuitively, the least favourable prior should make the alternative hypothesis hard to distinguish.
        One Sensible choice is concentrating $\Lambda$ on the boundary between $H_1$ and $H_0$
        (i.e. the limit $\{\sigma=\sigma_0\}$ ???).
        Thus $\Lambda$ will be a probability distribution over $\theta\in\mathbb{R}$ for the fixed $\sigma=\sigma_0$.
        
        Given any test function $\varphi(X)$ and a sufficient statistic $T$,
        thus exists a test function $\eta$ that has the same power as $\varphi$,
        but depends on $x$ only through $T$:
        \begin{gather}
            \eta(T(X))=\mathbb{E}[\varphi(X)|T(X)].
        \end{gather}

        Hence, we can restrict our attention to the sufficient Statistic $(Y,U)$,
        where $Y=\Bar{X}_n$ and $U=\sum_{i=1}^n(X_i-\Bar{X}_n)^2$.
        We know that $Y\sim\mathcal{N}(\theta,\frac{\sigma^2}{n})$,
        $U\sim\sigma^2\chi^2_{n-1}$ and $Y$ is independent of $U$ by Basu's Theorem.

        Thus, for $\Lambda$ supported on $\sigma=\sigma_0$,
        we have he joint density of $(Y,U)$ under $H_\Lambda$ as 
        \begin{gather}
            c_0 u^\frac{n-3}{2} \exp\left( -\frac{u}{2\sigma_0^2} \right)
            \int\exp\left\{ -\frac{u}{2\sigma_0^2}(y-u)^2 \right\}d\Lambda(\theta) \label{eq:comph01}
        \end{gather}
        and the joint density under the alternative hypothesis $(\theta,\sigma_1)$ as 
        \begin{gather}
            c_0 u^\frac{n-3}{2} \exp\left( -\frac{u}{2\sigma_1^2} \right)
            \exp\left\{ -\frac{n}{2\sigma_1^2}(y-\theta_1)^2 \right\}\label{eq:comph02}
        \end{gather}
        
        We see from Equations (\ref{eq:comph01}) and (\ref{eq:comph02}) 
        that the choice of $\Lambda$ only affects the distribution of $Y$.
        To achieve the maximum power against the alternative,
        we need to choose $\Lambda$ such that 
        the two distributions become as close as possible under $H_1:Y\sim\mathcal{N}(\theta,\frac{\sigma_1}{n})$.
        Under $H_1$, the distribution of $Y$ is in a convolution form:
        $Y=Z+\Theta$ for $Z\sim\mathcal{0,\frac{\sigma_0^2}{n}}$ and $\Theta\sim \Lambda$,
        where $\Theta$ and $Z$ are independent.
        Hence, if we choose $\Theta\sim\mathcal{N}(\theta_1,\frac{\sigma_1^2-\sigma_0^2}{n})$,
        $Y$ will have the same distribution under the null and the alternative,
        which is $\mathcal{N}(\theta_1,\frac{\sigma_1^2}{n})$.
        Under this choice of prior,
        the LRT rejects for large values of $\exp\left\{-\frac{u}{s\sigma_1^2}+\frac{u}{2\sigma_0^2}\right\}$,
        i.e. it rejects for large values of $u$ since $\sigma_1>\sigma_0$.
        So, the MP test rejects $H_\Lambda$ 
        if $\sum_{i=1}^n(X_i-\Bar{X}_n^2)$ lies above some threshold determined by the size constraint.
        In particular, it rejects if $\sum_{i=1}^n(X_i-\Bar{X}_n^2)>\sigma_0^2c_{n-1,1-\alpha}$,
        where $c_{n-1,1-\alpha}$ is the $(1-\alpha)$-th quantile of $\chi^2_{n-1}$.

        \item Next, we check if the MP test is level $\alpha$ for the composite null.
        For any $(\theta,\sigma)$ with $\sigma\leq\sigma_0$, the probability of rejection is 
        {\footnotesize\begin{gather}
            P_{\theta,\sigma}\left( 
                \frac{\sum_{i=1}^n(X_i-\Bar{X}_n)^2}{\sigma^2}
                > \frac{\sigma_0^2 c_{n-1,1-\alpha}}{\sigma^2} 
            \right)
            = P\left(
                \chi_{n-1}^2
                > \frac{\sigma_0^2}{\sigma^2} c_{n-1,1-\alpha}
            \right)
            \leq \alpha
        \end{gather}}
        where the equality holds if and only if $\sigma=\sigma_0$.
        Hence from Theorem \ref{thm:convertcompsim} (TSH 3.8.1) that our test is MP for testing the
        original null $H_0$ vs $\mathcal{N}(\theta_1,\sigma_1)$.
        \item Finally, the MP level-$\alpha$ test for testing 
        the composite null $H_0$ vs an arbitrarily chosen $(\theta_1,\sigma_1)$ does not depend on the choice of $(\theta_1,\sigma_1)$.
        Hence, it is UMP for testing the original composite null vs the composite alternatives.
    \end{enumerate}
\end{example}

\subsection{Duality between Testing and Interval Estimation}

\subsubsection{Confidence Interval}

Recall that a random set $S(X)$ is a $1-\alpha$ confidence region (or interval) 
for a parameter $\xi=\xi(\theta)$ if 
\begin{gather}
    P_\theta(\xi\in S(X))\geq 1-\alpha~~~\forall{\theta}\in\Omega
\end{gather}

For every $\xi_0$, let $A(\xi_0)$ be the acceptance region for non-randomized level-$\alpha$ test of 
$H_0:\xi(\theta)=\xi_0$ vs $H_1:\xi(\theta)\neq\xi_0$, so that 
\begin{gather}
    P_\theta(X\in A(\xi(\theta))) \geq 1-\alpha~~~\forall{\theta}\in\Omega.
\end{gather}
Define $S(x)=\{\xi:x\in A(\xi)\}$, then $\xi(\theta)\in S(X)$ if and only if $X\in A(\xi(\theta))$, so
\begin{gather}
    P_\theta(\xi(\theta)\in S(X))=P_\theta(X\in A(\xi(\theta)))\geq 1-\alpha.
\end{gather}
This shows that $S(X)$ is a $1-\alpha$ \textbf{confidence interval} (CI) for $\xi$.


\subsubsection{P-value}

For varying $\alpha$, the resulting tests provide an example of the typical situation
in which the rejection regions $S_\alpha$ are nested in the sense that 
\begin{gather}
    S_\alpha\subset S_{\alpha'}~\text{if}~\alpha<\alpha'.
\end{gather}
The quantity $\hat{p}=\hat{p}(X)=\inf\{\alpha:X\in S_\alpha\}$, 
the smallest significant level of observation, is called a p-value.
It gives an idea of how strongly the data contradict the null hypothesis.

\begin{example}
    Let $\Phi$ denote the standard normal cdf,
    then the rejection region can be written as 
    \begin{gather}
        S_\alpha
        = \{x:x>\sigma z_{1-\alpha}\}
        = \left\{x: \Phi(x/\sigma)>1-\alpha\right\}
        = \left\{x:1-\Phi(x/\sigma)<\alpha\right\}.
    \end{gather}
    For a given observed value of $X$, 
    the infimum over all $\alpha$ where the last inequality holds is $\hat{p}=1-\Phi(x/\sigma)$.

    Alternatively, the p-value is $p_0(X\geq x)$,
    where $x$ is the observed value of $X$.
    Note that under $\xi=0$, 
    the distribution of $\hat{p}$ is given by
    \begin{gather}
        P_0(\hat{p}\leq u)
        = P_0(1-\Phi(X/\sigma)\leq u)
        = P_0(\Phi(X/\sigma)\geq{1-u})=u
    \end{gather}
    $\Rightarrow\hat{p}$ is uniformly distributed on (0,1).\footnote{
    TSH Lemma 3.3.1 and Example 3.3.2
    }
\end{example}

\subsection{UMP}

\begin{enumerate}
    \item Impose a reasonable restriction on the tests to be considered 
    and the ``optimal'' tests within the class of tests under the restriction.
    \item Two typical strategies:
    \begin{enumerate}
        \item unbiasedness: UMP unbiased test
        \item invariance
    \end{enumerate}
\end{enumerate}

Recall that a UMP test $\varphi$ of size $\alpha$ has the property test 
\begin{gather}
    \mathbb{E}_\theta\varphi(X)\leq\alpha~\forall{\theta}\in\Omega_0
    ~\text{and}~
    \mathbb{E}_\theta\varphi(X)\geq\alpha~\forall{\theta}\in\Omega_1.
    \label{eq:ump}
\end{gather}
This means that $\varphi$ is at least as good as the naive test $\varphi\triangleq\alpha$.

\begin{definition}[Unbiased and Uniformly most powerful unbiased]
    Let $\alpha$ be a given level of significance. 
    A test $\varphi$ for $H_0:\theta\in\Omega_0$ vs $H_1:\theta\in\Omega_1$ is said to be \textbf{unbiased} of level $\alpha$,
    if and only if Equation (\ref{eq:ump}) holds.
    A test of size $\alpha$ is called a \textbf{uniformly most powerful unbiased} (UMPU) test,
    if and only if it is UMP within the class of unbiased tests of level $\alpha$.\footnote{
    Keener 12.7, Theorem 12.26 and Example 12.29.
    }
\end{definition}

More about \textbf{Consistency/Asymptotic Normality of MLE} referring to Keener Chapter 9 (EE, EM algorithms)






